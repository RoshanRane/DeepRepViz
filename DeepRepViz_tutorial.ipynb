{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoshanRane/DeepRepViz/blob/main/DeepRepViz_tutorial.ipynb\"> <img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open in Google Colaboratory\"></a><a href=\"https://github.com/RoshanRane/DeepRepViz\"><img align=\"left\" src=\"https://img.shields.io/badge/Github-Download-blue.svg\" alt=\"Download\" title=\"Download Notebook\"></a><a href=\"https://link.springer.com/chapter/10.1007/978-3-031-72117-5_18\"><img align=\"left\" src=\"https://img.shields.io/badge/Read%20the%20Paper-8A2BE2\" alt=\"Download\" title=\"Download Notebook\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:red\">**Alert**</span>\n",
        "  - Click on `Runtime` in the top menu.<br>\n",
        "  - Select `Change runtime type` from the dropdown.<br>\n",
        "  - In the \"Hardware accelerator\" dropdown, select `T4 GPU`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Install dependencies <a name=\"install\"></a>\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Function to install a package\n",
        "def install(package):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\"] + package.split())\n",
        "\n",
        "# List of packages to check and install\n",
        "packages = {\n",
        "    \"numpy\": \"numpy\",\n",
        "    \"pandas\": \"pandas\",\n",
        "    \"matplotlib\": \"matplotlib\",\n",
        "    \"statsmodels\": \"statsmodels\",\n",
        "    \"tqdm\": \"tqdm\",\n",
        "    \"torch\": \"torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\",\n",
        "    \"einops\": \"einops\",\n",
        "    \"lightning\": \"lightning\",\n",
        "    \"dcor\": \"dcor\",\n",
        "}\n",
        "\n",
        "# Loop through packages and check if they need to be installed\n",
        "for lib, install_name in packages.items():\n",
        "    try:\n",
        "        __import__(lib)\n",
        "    except ImportError:\n",
        "        print(f\"{lib} not found, installing...\")\n",
        "        install(install_name)\n",
        "\n",
        "!pip install gpustat\n",
        "! gpustat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Download source files from github\n",
        "!git clone https://github.com/RoshanRane/DeepRepViz.git\n",
        "!unzip -qq /content/DeepRepViz/data/toybrains_n5000_lblmidr-consite_cy025-cX025-yX050.zip -d /content/DeepRepViz/data\n",
        "!rm /content/DeepRepViz/data/toybrains_n5000_lblmidr-consite_cy025-cX025-yX050.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Roshan Prakash Rane***<br>\n",
        "​-----------<br>\n",
        "PhD Candidate<br>\n",
        "Department of Psychiatry and Neurosciences, Charité - Universitätsmedizin Berlin, Berlin, Germany<br>\n",
        "Department of Psychology, Humboldt-Universität zu Berlin, Berlin, Germany<br>\n",
        "<br>\n",
        "***JiHoon Kim***<br>\n",
        "​-----------<br>\n",
        "PhD Candidate<br>\n",
        "Max Planck Institute for Human Cognitive and Brain Sciences, Leipzig, Germany<br>\n",
        "INM-7, Reseasrch Centre Jülich, Jülich, Germany<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DeepRepViz Tutorial Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- [Tutorial Objective](#objective)\n",
        "- [A Guide to Implementing DeepRepViz in Your Predictive Modeling](#guide)\n",
        "  - [1. Use the DeepRepViz Callback](#use)\n",
        "  - [2. Train your model](#train)\n",
        "  - [3. Compute Metrics with DeepRepVizBackend](#compute)\n",
        "  - [4. Find the Generated File for Web-Based Visualization](#generate)\n",
        "  - [Interim summary](#interim)\n",
        "\n",
        "- [Usecase](#usecase)\n",
        "  - [Import packages](#packages)\n",
        "  - [Dataset](#dataset)\n",
        "  - [Utils](#utils)\n",
        "  - [Configuration](#config)\n",
        "  - [Main](#main)\n",
        "- [References](#ref)\n",
        "- [Acknolwedgements and Funding](#ack&funding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tutorial Objective <a name=\"objective\"></a>\n",
        "\n",
        "*Estimated timing of tutorial: 20 minutes*\n",
        "\n",
        "This notebook demonstrates that how DeeRepViz can be integrated into a predictive deep learning application\n",
        "\n",
        "In this tutorial, we will show how to use [DeepRepViz](https://link.springer.com/chapter/10.1007/978-3-031-72117-5_18) using [Toy Brains Dataset](https://github.com/RoshanRane/toybrains)\n",
        "- Learn how to use DeepRepViz callback\n",
        "- Compute con score and generate the files using DeepRepVizBackend for [online visualization tool](https://deep-rep-viz.vercel.app/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## A Guide to Implementing DeepRepViz in Your Predictive Modeling <a name=\"guide\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Callback](https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html) in PyTorch Lightning allows you to add custom logic at various training steps, enhancing metrics monitoring and modifying behavior. They enable you to hook useful information during the training process.\n",
        "\n",
        "### 1. Use the DeepRepViz Callback <a name=\"use\"></a>\n",
        "\n",
        "To incorporate the DeepRepViz callback, instantiate it and pass it to the Trainer:\n",
        "\n",
        "```\n",
        "from deeprepviz.callback import DeepRepViz\n",
        "import lightning as L\n",
        "\n",
        "# Initialize DeepRepViz callback\n",
        "drv = DeepRepViz(...)\n",
        "\n",
        "# Train model\n",
        "trainer = L.Trainer(callbacks=[drv])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Train your model <a name=\"train\"></a>\n",
        "\n",
        "Start the training process with `trainer.fit(model, datamodule=data_module)` to train with the callback enabled."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Compute Metrics with DeepRepVizBackend <a name=\"compute\"></a>\n",
        "\n",
        "The DeepRepVizBackend automatically computes metrics like the `Con-score`, and generates files for the web-based visualization tool. This requires the `raw_csv` and the Trainer. For instructions on obtaining the `raw_csv`, please refer to the [documentation](https://deep-rep-viz.vercel.app/docs.html).\n",
        "```\n",
        "from deeprepviz.backend import DeepRepvizBackend\n",
        "\n",
        "# Initialize DeepRepViz Backend\n",
        "drv_backend = DeepRepVizBackend(...)\n",
        "log_dir = trainer.log_dir + '/deeprepvizlog/'\n",
        "drv_backend.load_log(log_dir)\n",
        "\n",
        "# Compute and generate the files\n",
        "drv_backend.convert_log_to_v1_table(log_key=log_dir)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Find the Generated File for Web-Based Visualization <a name=\"generate\"></a>\n",
        "\n",
        "After computing metrics with the DeepRepVizBackend, you can find the generated files in the `log/your-dataset-name_your-model-name/trial_*/deeprepvizlog/` folder. Look for files named `DeepRepViz-*.csv`.\n",
        "\n",
        "Once you have the file, you can upload it to the web-based visualization tool. For detailed instructions on using the tool, please refer to the [documentation](https://deep-rep-viz.vercel.app/docs.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interim summary <a name=\"interim\"></a>\n",
        "We use a `DeepRepViz` callback to capture useful information during training, while the `DeepRepVizBackend` computes the Con-score metric and generates files for [the web-based visualization tool](https://deep-rep-viz.vercel.app/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Usecase <a name=\"usecase\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRChQ0KyuYvP"
      },
      "outputs": [],
      "source": [
        "# @title Import packages <a name=\"packages\"></a>\n",
        "\n",
        "# import standard python packages\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import datetime\n",
        "from glob import glob\n",
        "import lightning as L\n",
        "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
        "from lightning.pytorch.loggers import TensorBoardLogger\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchmetrics\n",
        "from torchvision import transforms\n",
        "from typing import Dict, List, Type\n",
        "import warnings\n",
        "\n",
        "sys.path.append(os.path.abspath('/content/DeepRepViz'))\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "# import DeepRepViz packages\n",
        "from deeprepviz.utils import D2metric\n",
        "from deeprepviz.callback import (\n",
        "    DeepRepViz,\n",
        "    get_all_model_layers,\n",
        "    get_param_count\n",
        "    )\n",
        "from deeprepviz.backend import DeepRepVizBackend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Dataset <a name=\"dataset\"></a>\n",
        "\n",
        "PATHS = \"/content/DeepRepViz/data/toybrains_*\"\n",
        "DATASETS = sorted([os.path.abspath(path) for path in glob(PATHS)])\n",
        "print(f\"Fitting DL model on the following toybrains datasets:\\n{DATASETS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Utils <a name=\"utils\"></a>\n",
        "\n",
        "# DataLoader\n",
        "class ToyBrainsDataloader(Dataset):\n",
        "  def __init__(self, img_dir, img_names, labels, transform=None):\n",
        "    self.img_dir = img_dir\n",
        "    self.img_names = img_names\n",
        "    self.labels = labels\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    number = str(self.img_names[index]).zfill(5)\n",
        "    img = Image.open(os.path.join(self.img_dir, number + \".jpg\"))\n",
        "\n",
        "    if self.transform is not None:\n",
        "      img = self.transform(img)\n",
        "\n",
        "    label = torch.as_tensor(self.labels[int(index)]).type(torch.LongTensor)\n",
        "    return img, label\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.labels.shape[0]\n",
        "\n",
        "# Model\n",
        "class ConvBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2),\n",
        "    )\n",
        "\n",
        "    self._init_weights()\n",
        "\n",
        "  def _init_weights(self):\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight)\n",
        "        if m.bias is not None:\n",
        "          nn.init.zeros_(m.bias)\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        nn.init.constant_(m.weight, 1)\n",
        "        nn.init.zeros_(m.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    return x\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "  def __init__(self, num_classes, final_act_size=64):\n",
        "    super().__init__()\n",
        "    self.final_act_size = final_act_size # weights + 1 bias\n",
        "    # convolutional layers\n",
        "    self.conv = nn.Sequential(\n",
        "        ConvBlock(in_channels=3, out_channels=16),\n",
        "        nn.Dropout(0.1),\n",
        "        ConvBlock(in_channels=16, out_channels=32),\n",
        "        nn.Dropout(0.1),\n",
        "        ConvBlock(in_channels=32, out_channels=64),\n",
        "    )\n",
        "\n",
        "    # fully connected layers\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        # TODO hardcoded input size\n",
        "        nn.Linear(64 * 8 * 8, self.final_act_size, bias=True),\n",
        "        nn.Linear(self.final_act_size, num_classes, bias=True),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n",
        "# Lightning Module\n",
        "class LightningModel(L.LightningModule):\n",
        "    def __init__(self, model, learning_rate,\n",
        "                 task=\"binary\", num_classes=1):\n",
        "        '''LightningModule that receives a PyTorch model as input'''\n",
        "        super().__init__()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.model = model\n",
        "        self.num_classes = num_classes\n",
        "        # self.metric_acc = torchmetrics.Accuracy(task=task, num_classes=num_classes)\n",
        "        self._metric_spec = torchmetrics.Specificity(task=task, num_classes=num_classes)\n",
        "        self._metric_recall = torchmetrics.Recall(task=task, num_classes=num_classes)\n",
        "        self.metric_D2 = D2metric()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def _shared_step(self, batch):\n",
        "        features, true_labels = batch\n",
        "        logits = self(features)\n",
        "        # compute all metrics on the predictions\n",
        "        if self.num_classes==1:\n",
        "            logits = torch.squeeze(logits, dim=-1)\n",
        "            true_labels = true_labels.to(torch.float32)\n",
        "            # print(logits.shape, true_labels.shape)\n",
        "            loss = F.binary_cross_entropy_with_logits(logits, true_labels)\n",
        "            predicted_labels = torch.sigmoid(logits)>0.5\n",
        "\n",
        "        else:\n",
        "            loss = F.cross_entropy(logits, true_labels)\n",
        "            predicted_labels = torch.argmax(logits, dim=1)\n",
        "        # acc = self.metric_acc(predicted_labels, true_labels)\n",
        "        # calculate balanced accuracy\n",
        "        spec = self._metric_spec(predicted_labels, true_labels)\n",
        "        recall = self._metric_recall(predicted_labels, true_labels)\n",
        "        BAC = (spec+recall)/2\n",
        "        D2 = self.metric_D2(logits, true_labels)\n",
        "        metrics = {'loss':loss, 'BAC':BAC, 'D2':D2}\n",
        "        return true_labels, logits, metrics\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        labels, preds, metrics = self._shared_step(batch)\n",
        "        # append 'train_' to every key\n",
        "        log_metrics = {'train_'+k:v for k,v in metrics.items()}\n",
        "        self.log_dict(log_metrics,\n",
        "                      prog_bar=True,\n",
        "                      on_epoch=True, on_step=False)\n",
        "        return log_metrics['train_loss']\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        labels, preds, metrics = self._shared_step(batch)\n",
        "        # append 'val_' to every key\n",
        "        log_metrics = {'val_'+k:v for k,v in metrics.items()}\n",
        "        self.log_dict(log_metrics,\n",
        "                      prog_bar=True,\n",
        "                      on_epoch=True, on_step=False)\n",
        "        return labels, preds\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        labels, preds, metrics = self._shared_step(batch)\n",
        "        # append 'val_' to every key\n",
        "        log_metrics = {'test_'+k:v for k,v in metrics.items()}\n",
        "        self.log_dict(log_metrics,\n",
        "                      prog_bar=True,\n",
        "                      on_epoch=True, on_step=False)\n",
        "        return labels, preds\n",
        "\n",
        "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        # DeepRepViz returns ids and the normal batch outputs as tuples\n",
        "        ids, batch = batch\n",
        "        true_labels, logits, metrics = self._shared_step(batch)\n",
        "        return ids, true_labels, logits, metrics\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        opt = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min',\n",
        "                                                         factor=0.75, patience=3)\n",
        "        return {\n",
        "            \"optimizer\": opt,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": sch,\n",
        "                \"monitor\": \"val_loss\",\n",
        "                \"interval\": \"epoch\", # default\n",
        "                \"frequency\": 1, # default\n",
        "            },\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Configuration <a name=\"config\"></a>\n",
        "@dataclass\n",
        "class Config:\n",
        "  \"\"\"Configuration options for the Ligtning ToyBrains example.\n",
        "\n",
        "  Attributes:\n",
        "      random_seed (int or None): Seed for random number generation.\n",
        "      debug (bool): Flag for enabling debug mode.\n",
        "      ID_COL (str): Column name for ID.\n",
        "      trial (str): Trial identifier.\n",
        "      LABEL_COL (str): Column name for labels.\n",
        "      additional_drv_test_data (Dict): Additional test data for DRV.\n",
        "      model_class (Type): Class of the model to use.\n",
        "      model_kwargs (Dict): Additional keyword arguments for the model.\n",
        "      learning_rate (float): Learning rate for the optimizer.\n",
        "      unique_name (str): Unique name for the experiment.\n",
        "      additional_loggers (List): List of additional loggers.\n",
        "      additional_callbacks (List): List of additional callbacks.\n",
        "      early_stop_patience (int): Patience for early stopping.\n",
        "      batch_size (int): Batch size for training.\n",
        "      num_workers (int): Number of workers for data loading.\n",
        "      gen_v1_table (bool): Flag to generate version 1 table.\n",
        "      k_fold (int): Number of folds for cross-validation.\n",
        "      no_ood_val (bool): Flag to disable out-of-distribution validation.\n",
        "      data_dir (str): Directory path for data.\n",
        "      trainer_args (Dict): Additional arguments for the trainer.\n",
        "\n",
        "  Examples:\n",
        "      A new instance of this dataclass can be created as follows:\n",
        "\n",
        "      >>> config = Config()\n",
        "\n",
        "      The default values for each argument are shown in the document above. If desired, any of these values can be overriden when creating a new instance of the dataclass:\n",
        "\n",
        "      >>> config = Config(batch_size=128, max_epochs=5)\n",
        "\n",
        "  \"\"\"\n",
        "  random_seed: int = None\n",
        "  debug: bool = False\n",
        "  trial: str = 'trial_0'\n",
        "  no_ood_val: bool = True\n",
        "  id_col: str = 'subjectID'\n",
        "  label_col: str = 'lbl_lesion'\n",
        "  split_col: str = \"datasplit\"\n",
        "  data_dir: str = '/content/DeepRepViz/data/toybrains_n5000_lblmidr-consite_cy025-cX025-yX050'\n",
        "  additional_drv_test_data: Dict = field(default_factory=dict)\n",
        "  model_class = SimpleCNN\n",
        "  model_kwargs: Dict = field(default_factory=lambda: dict(num_classes=1, final_act_size=65))\n",
        "  learning_rate: float = 0.03\n",
        "  unique_name: str = ''\n",
        "  additional_loggers: List = field(default_factory=list)\n",
        "  additional_callbacks: List = field(default_factory=list)\n",
        "  early_stop_patience: int = 8\n",
        "  batch_size: int = 64\n",
        "  num_workers: int = 2\n",
        "  k_fold: int = 1\n",
        "  trainer_args: Dict = field(default_factory=lambda: dict(max_epochs=10, accelerator='gpu', devices=[0]))\n",
        "  gen_v1_table: bool = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_time = datetime.now()\n",
        "\n",
        "# Set the configuration\n",
        "config = Config()\n",
        "\n",
        "# Forcefully set a random seed in debug mode\n",
        "random_seed = 42\n",
        "\n",
        "if config.random_seed is None and config.debug:\n",
        "  random_seed = 42\n",
        "\n",
        "if config.random_seed is not None:\n",
        "  random_seed = config.random_seed\n",
        "  torch.manual_seed(random_seed)\n",
        "  np.random.seed(random_seed)\n",
        "  random.seed(random_seed)\n",
        "  L.seed_everything(random_seed)\n",
        "\n",
        "unique_name = config.unique_name\n",
        "\n",
        "if config.debug:\n",
        "  os.system('rm -rf log/*debugmode*')\n",
        "  unique_name = 'debugmode'+unique_name\n",
        "  config.trainer_args['max_epochs'] = 2 if config.trainer_args['max_epochs'] > 100 else config.trainer_args['max_epochs']\n",
        "  batch_size = 5\n",
        "  k_fold = 2 if config.k_fold>2 else config.k_fold\n",
        "  num_workers = 5\n",
        "  no_ood_val = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "DATA_DIR = os.path.abspath(config.data_dir)\n",
        "DATA_CSV = glob(DATA_DIR + '/toybrains*.csv')\n",
        "assert len(DATA_CSV)==1, f\"Toybrains dataset found = {DATA_CSV}.\\\n",
        "\\nEnsure that that the dataset {config.data_dir} is generated using the `create_toybrains.py` script in the toybrains repo. \\\n",
        "Also ensure only one dataset exists for the given query '{DATA_DIR}'.\"\n",
        "DATA_CSV = DATA_CSV[0]\n",
        "N_SAMPLES = int(DATA_DIR.split('_n')[-1].split('_')[0])\n",
        "\n",
        "# Collect the corresponding OOD test data\n",
        "OOD_test_datasets = {}\n",
        "if not config.no_ood_val:\n",
        "  test_suffix = '_test'# Hardcoded\n",
        "  test_nsamples = 1000 # Hardcoded: n samples of toybrains test datasets are 1000\n",
        "  data_dir_test = re.sub(f'_n{N_SAMPLES}_', f'_n{test_nsamples}_', DATA_DIR) + test_suffix\n",
        "  data_dir_test_noconf = re.sub('cX...', 'cX000', re.sub('cy...','cy000', data_dir_test))\n",
        "  assert os.path.exists(data_dir_test_noconf), f\"Could not find the equivalent 'no-conf' dataset {data_dir_test_noconf} for {DATA_DIR}\"\n",
        "\n",
        "  data_dir_test_notrue = re.sub('yX...','yX000', data_dir_test)\n",
        "  assert os.path.exists(data_dir_test_notrue), f\"Could not find the equivalent 'no-true' dataset {data_dir_test_notrue} for {dataset}\"\n",
        "\n",
        "  OOD_test_datasets = {'test-no-conf': data_dir_test_noconf, 'test-no-true': data_dir_test_notrue}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare the data splits as a dataframe mapping the subjectID to the split and trial\n",
        "data = pd.read_csv(DATA_CSV)\n",
        "ID_COL, LABEL_COL, SPLIT_COL = config.id_col, config.label_col, config.split_col\n",
        "assert ID_COL in data.columns, f\"ID_COL={ID_COL} is not present in the dataset's csv file. \\\n",
        "Available colnames = {data.columns.tolist()}\"\n",
        "assert LABEL_COL in data.columns, f\"LABEL_COL={LABEL_COL} is not present in the dataset's csv file. \\\n",
        "Available colnames = {data.columns.tolist()}\"\n",
        "\n",
        "## SPLITS: Create the n-fold splits for the data\n",
        "# Drop all columns except subjectID and label\n",
        "datasplit_df = data.drop(columns=[c for c in data.columns if c not in [ID_COL, LABEL_COL]])\n",
        "datasplit_df = datasplit_df.set_index(ID_COL)\n",
        "# Create 'trial_x' columns: init as columns as args.k_fold\n",
        "for trial in range(config.k_fold):\n",
        "  datasplit_df[f'trial_{trial}'] = 'unknown'\n",
        "# First, set aside 20% of the data as test and assign it commonly to all folds\n",
        "train_idxs, test_idxs = train_test_split(datasplit_df.index, test_size=0.2,\n",
        "                                         random_state=random_seed)\n",
        "for trial in range(config.k_fold):\n",
        "  datasplit_df.loc[test_idxs, f'trial_{trial}'] = 'test'\n",
        "\n",
        "if config.k_fold <= 1: # if 1 fold then initialize all data to the first trial\n",
        "  train_idxs, val_idxs = train_test_split(train_idxs, test_size=0.1,\n",
        "                                          random_state=random_seed)\n",
        "  datasplit_df.loc[train_idxs, 'trial_0'] = 'train'\n",
        "  datasplit_df.loc[val_idxs, 'trial_0'] = 'val'\n",
        "else: # if k-fold then split the data into k times and assign each to a sep trial\n",
        "  splitter = StratifiedKFold(n_splits=k_fold,\n",
        "                             shuffle=True,\n",
        "                             random_state=random_seed)\n",
        "  splits = splitter.split(train_idxs, y=datasplit_df.loc[train_idxs, LABEL_COL])\n",
        "  for trial_idx, (train_idxs_i, val_idxs_i) in enumerate(splits):\n",
        "      datasplit_df.loc[train_idxs[train_idxs_i], f'trial_{trial_idx}'] = 'train'\n",
        "      datasplit_df.loc[train_idxs[val_idxs_i], f'trial_{trial_idx}'] = 'val'\n",
        "\n",
        "datasplit_df = datasplit_df.sort_index()\n",
        "(datasplit_df.filter(like='trial')!='unknown').all(), \"some data points are not assigned to any split. {}\".format(datasplit_df)\n",
        "\n",
        "dataset_name = os.path.basename(DATA_DIR)\n",
        "\n",
        "# Split the dataset as defined in the datasplit_df\n",
        "if datasplit_df.index.name==ID_COL: datasplit_df = datasplit_df.reset_index()\n",
        "# Select a specific trial given by 'trial' out of the k-folds\n",
        "trial = f'trial_{trial}'\n",
        "\n",
        "datasplit_df = datasplit_df.rename(columns={trial:SPLIT_COL})\n",
        "datasplit_df = datasplit_df[[ID_COL, LABEL_COL, SPLIT_COL]]\n",
        "\n",
        "df_train = datasplit_df[datasplit_df[SPLIT_COL]=='train']\n",
        "df_val = datasplit_df[datasplit_df[SPLIT_COL]=='val']\n",
        "df_test = datasplit_df[datasplit_df[SPLIT_COL]=='test']\n",
        "\n",
        "print(f\"Dataset: {dataset_name} \\n  Training data split = {len(df_train)} \\n\\\n",
        "Validation data split = {len(df_val)} \\n  Test data split = {len(df_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create pytorch data loaders\n",
        "train_dataset = ToyBrainsDataloader(\n",
        "  img_names = df_train[ID_COL].values, # TODO change hardcoded\n",
        "  labels = df_train[LABEL_COL].values,\n",
        "  img_dir = DATA_DIR+'/images',\n",
        "  transform = transforms.Compose([transforms.ToTensor()])\n",
        "  )\n",
        "train_loader = DataLoader(\n",
        "  dataset=train_dataset,\n",
        "  shuffle=True, batch_size=config.batch_size, drop_last=True,\n",
        "  num_workers=config.num_workers,\n",
        "  )\n",
        "\n",
        "val_dataset = ToyBrainsDataloader(\n",
        "  img_names = df_val[ID_COL].values,\n",
        "  labels = df_val[LABEL_COL].values,\n",
        "  img_dir = DATA_DIR+'/images',\n",
        "  transform = transforms.Compose([transforms.ToTensor()])\n",
        "  )\n",
        "val_loader = DataLoader(\n",
        "  dataset=val_dataset,\n",
        "  shuffle=False, batch_size=config.batch_size, drop_last=True,\n",
        "  num_workers=config.num_workers,\n",
        "  )\n",
        "\n",
        "# Create dataloaders for DeepRepViz() with no shuffle\n",
        "drv_train_dataset = {\n",
        "    'dataloader_kwargs': dict(img_dir=DATA_DIR+'/images',\n",
        "                                img_names=df_train[ID_COL].values,\n",
        "                                labels=df_train[LABEL_COL].values,\n",
        "                                transform=transforms.ToTensor()),\n",
        "    \"expected_IDs\":df_train[ID_COL].values,\n",
        "    \"expected_labels\":df_train[LABEL_COL].values,\n",
        "    }\n",
        "drv_test_datasets = {\n",
        "    'val': {\n",
        "          'dataloader_kwargs': dict(img_dir=DATA_DIR+'/images',\n",
        "                                    img_names=df_val[ID_COL].values,\n",
        "                                    labels=df_val[LABEL_COL].values,\n",
        "                                    transform=transforms.ToTensor()),\n",
        "          \"expected_IDs\":df_val[ID_COL].values,\n",
        "          \"expected_labels\":df_val[LABEL_COL].values\n",
        "          },\n",
        "    'test': {\n",
        "          'dataloader_kwargs': dict(img_dir=DATA_DIR+'/images',\n",
        "                                    img_names=df_test[ID_COL].values,\n",
        "                                    labels=df_test[LABEL_COL].values,\n",
        "                                    transform=transforms.ToTensor()),\n",
        "          \"expected_IDs\":df_test[ID_COL].values,\n",
        "          \"expected_labels\":df_test[LABEL_COL].values\n",
        "          }\n",
        "      }\n",
        "# Append any additional test datasets provided too\n",
        "for testdata_name, testdata_path in config.additional_drv_test_data.items():\n",
        "  testdata_csv = glob(testdata_path + '/toybrains*.csv')\n",
        "  assert len(testdata_csv)==1, f\"Toybrains Test dataset found = {testdata_csv} in the path {testdata_path} ..\"\n",
        "  testdata_df = pd.read_csv(testdata_csv[0])\n",
        "\n",
        "  drv_test_datasets[testdata_name] = {\n",
        "        'dataloader_kwargs': dict(img_dir=testdata_path+'/images',\n",
        "                                  img_names=testdata_df[ID_COL].values,\n",
        "                                  labels=testdata_df[LABEL_COL].values,\n",
        "                                  transform=transforms.ToTensor()),\n",
        "        \"expected_IDs\":testdata_df[ID_COL].values,\n",
        "        \"expected_labels\":testdata_df[LABEL_COL].values\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model\n",
        "model = config.model_class(**config.model_kwargs)\n",
        "lightning_model = LightningModel(model, learning_rate=config.learning_rate,\n",
        "                                   num_classes=config.model_kwargs['num_classes'])\n",
        "\n",
        "# Configure TensorBoardLogger as the main logger\n",
        "# Create a unique name for the logs based on the dataset, model and user provided suffix\n",
        "if unique_name != '': unique_name = '_' + unique_name\n",
        "unique_name = f'{dataset_name}_{config.model_class.__name__}{unique_name}'\n",
        "logger = TensorBoardLogger(save_dir='log', name=unique_name, version=trial)\n",
        "if config.additional_loggers: # plus, any additional user provided loggers\n",
        "  logger = [logger] + config.additional_loggers\n",
        "\n",
        "print(f\"pytorch_total_params = {get_param_count(model)}\")\n",
        "print(get_all_model_layers(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## DeepRepViz Callback\n",
        "# Initalize DeepRepViz callback\n",
        "drv = DeepRepViz(dataloader_class=ToyBrainsDataloader,\n",
        "                 dataset_kwargs=drv_train_dataset,\n",
        "                 datasets_kwargs_test=drv_test_datasets,\n",
        "                 hook_layer=-1,\n",
        "                 best_ckpt_by='loss_val', best_ckpt_metric_should_be='min',\n",
        "                 verbose=int(config.debug))\n",
        "\n",
        "callbacks = config.additional_callbacks + [drv]\n",
        "# Add any other callbacks\n",
        "if config.early_stop_patience:\n",
        "  callbacks.append(EarlyStopping(monitor=\"val_loss\", mode=\"min\",\n",
        "                                 patience=config.early_stop_patience))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train model\n",
        "trainer = L.Trainer(callbacks=callbacks,\n",
        "                      logger=logger,\n",
        "                      overfit_batches = 5 if config.debug else 0,\n",
        "                      log_every_n_steps= 2 if config.debug else 50,\n",
        "                      **config.trainer_args) # deterministic=True\n",
        "trainer.fit(\n",
        "    model=lightning_model,\n",
        "    train_dataloaders=train_loader,\n",
        "    val_dataloaders=val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## DeepRepViz Backend\n",
        "# Create the DeepRepViz v1 table\n",
        "if config.gen_v1_table:\n",
        "  raw_csv_path = glob(f'{DATA_DIR}/*{dataset_name}.csv')[0]\n",
        "\n",
        "df_data = pd.read_csv(raw_csv_path)\n",
        "drv_backend = DeepRepVizBackend(\n",
        "    conf_table=df_data,\n",
        "    best_ckpt_by='loss_val',\n",
        "    ID_col=ID_COL, label_col=LABEL_COL)\n",
        "\n",
        "log_dir = trainer.log_dir + '/deeprepvizlog/'\n",
        "drv_backend.load_log(log_dir)\n",
        "\n",
        "# Downsample the activations to 3D if not already done\n",
        "drv_backend.downsample_activations()\n",
        "\n",
        "drv_backend.convert_log_to_v1_table(log_key=log_dir, unique_name=unique_name)\n",
        "drv_backend.debug = False\n",
        "\n",
        "metrics = ['dcor', 'mi', 'con', 'costeta', 'r2']\n",
        "existing_metrics = drv_backend.get_metrics(log_dir, ckpt_idx='best')\n",
        "\n",
        "if existing_metrics is not None:\n",
        "  metrics = [m for m in metrics if m not in existing_metrics]\n",
        "  print(f\"Skipping {list(existing_metrics.keys())} for {log_dir}. As they have already been computed.\")\n",
        "\n",
        "# Compute and store the metrics in the metametadata.json file of the log_dir\n",
        "if len(metrics) > 0:\n",
        "  result = drv_backend.compute_metrics(log_key=log_dir,\n",
        "                                       metrics=metrics,\n",
        "                                       #   covariates=['lbl_lesion','cov_site', 'brain-int_fill','shape-midr_curv', 'shape-midr_vol-rad'],\n",
        "                                       ckpt_idx='best')\n",
        "\n",
        "total_time = datetime.now() - start_time\n",
        "minutes, seconds = divmod(total_time.total_seconds(), 60)\n",
        "print(f\"Total runtime: {int(minutes)} minutes {int(seconds)} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the `log/your-dataset-name_your-model-name/trial_*/deeprepvizlog/` folder, locate the `DeepRepViz-*.csv` file. After downloading it, upload the file to the web-based visualization tool. For detailed instructions, visit the [documentation page](https://deep-rep-viz.vercel.app/docs.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summarize the result\n",
        "all_results = {}\n",
        "logdirs = sorted([log for log in glob(f\"log/toybrains_*/*/deeprepvizlog/\") if 'debug' not in log])\n",
        "print(logdirs)\n",
        "\n",
        "for logdir in logdirs:\n",
        "    print(\"loading:\", logdir)\n",
        "    drv_backend = DeepRepVizBackend()\n",
        "    drv_backend.load_log(logdir)\n",
        "    log = drv_backend.deeprepvizlogs[logdir]\n",
        "    ckpt_idx = log['best_ckpt_idx']\n",
        "    ckptname, log_ckpt = log['checkpoints'][ckpt_idx]\n",
        "    logdirname = logdir.split('/')[-4].replace('toybrains-','')\n",
        "    model_setting = logdirname.split('_')[-1]\n",
        "    logdirname = logdirname.replace('_'+model_setting, '')\n",
        "    # print('='*100,'\\n', method_name, \"at ckpt =\", ckptname)\n",
        "    # print(log.keys())\n",
        "    # print(\"Model accuracy =\", log_ckpt['metrics'])\n",
        "    result = {(\"Model\",k): v for k,v in log_ckpt['metrics'].items()}\n",
        "    for metric_name, metric_scores in log_ckpt['act_metrics'].items():\n",
        "        # print('-'*100,\"\\nMetric =\", metric_name, '\\n', '-'*100,)\n",
        "        for key in ['lbl_lesion', 'cov_site', 'brain-int_fill', 'shape-midr_curv', 'shape-midr_vol-rad']:\n",
        "            result.update({(key, metric_name): metric_scores[key]})\n",
        "            # print(\"{} = {:.4f}\".format(key, metric_scores[key]))\n",
        "\n",
        "    all_results.update({(model_setting, logdirname): result})\n",
        "\n",
        "df_results = pd.DataFrame.from_dict(all_results, orient='index')\n",
        "# Sort the dataframe by the two levels of column headers\n",
        "df_results = df_results.sort_index(axis=1, level=[0,1]).sort_index()\n",
        "df_results.style.bar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References <a name=\"ref\"></a>\n",
        "\n",
        "> Rane, R.P., Kim, J., Umesha, A., Stark, D., Schulz, M.A., Ritter, K. (2024). **DeepRepViz: Identifying Potential Confounders in Deep Learning Model Predictions** *Medical Image Computing and Computer Assisted Intervention - MICCAI 2024*, pp 186-196.\n",
        "\n",
        "> Have a look at toybrains dataset and tutorial: https://github.com/RoshanRane/toybrains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Acknowledgements and Funding <a name=\"ack&funding\"></a>\n",
        "\n",
        "> This project was inspired by Google Brain's [projector.tensorflow.org](https://projector.tensorflow.org/), but is more catering towards the medical domain and medical imaging analysis. For implementation, we heavily rely on [3D-scatter-plot from plotly.js](https://plotly.com/javascript/3d-scatter-plots/).\n",
        "\n",
        "> This project was funded by the DeSBi Research Unit (DFG; KI-FOR 5363; Profject ID 459422098), the consortium SFB/TRR 265 Losing and Regaining Control over Drug Intake (DFG; Project ID 402170461), FONDA (DFG; SFB 1404; Project ID: 414984028) and FOR 5187 (DFG; Project ID: 442075332)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMV7VtLKPHxGLhZT97H8wFQ",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
